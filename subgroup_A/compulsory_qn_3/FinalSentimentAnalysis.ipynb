{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code details the process of tokenizing the words from the reviews and extracting a sentiment using NLTK packages and resources. The analysis is further strengthened through the use of pre-trained word embeddings and"
      ],
      "metadata": {
        "id": "jjW6O8ycFtzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M3CrmHc3FvRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKvF8JiGWCdF",
        "outputId": "6cbc6627-fcc8-4b42-8846-e162715e7ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/DSA3101 Data/Clean Data/dataset_tripadvisor-reviews_2024_cleaned.xlsx')\n",
        "\n",
        "# Initialize nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Define sentiment dictionaries\n",
        "positive_words = ['good', 'excellent', 'lovely', 'amazing', 'perfect', 'awesome']\n",
        "negative_words = ['bad', 'expensive', 'rude', 'terrible', 'poor', 'awful']\n",
        "neutral_words = ['average', 'okay', 'fine', 'decent', 'satisfactory']\n",
        "\n",
        "# Function to get word vector from embeddings\n",
        "def get_word_vector(word):  # **New Change: Get word vector function**\n",
        "    return glove[word] if word in glove else np.zeros(glove.vector_size)\n",
        "\n",
        "# Function to preprocess text and tokenize\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]  # Remove stopwords and lemmatize\n",
        "    word_vectors = [get_word_vector(word) for word in tokens]\n",
        "    return tokens, word_vectors  # Return both tokens and their vectors\n",
        "\n",
        "# Function to handle mixed feelings around \"but\"\n",
        "def handle_but(tokens):\n",
        "    if 'but' in tokens:\n",
        "        but_index = tokens.index('but')\n",
        "        before_but = tokens[:but_index]\n",
        "        after_but = tokens[but_index + 1:]\n",
        "        return before_but, after_but\n",
        "    else:\n",
        "        return tokens, []\n",
        "\n",
        "# Function to handle negations\n",
        "def handle_negations(tokens):\n",
        "    negations = [\"not\", \"no\", \"never\", \"isn't\", \"wasn't\"]\n",
        "    new_tokens = []\n",
        "    negate = False\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in negations:\n",
        "            negate = True\n",
        "        else:\n",
        "            if negate:\n",
        "                new_tokens.append(\"not_\" + word)\n",
        "                negate = False\n",
        "            else:\n",
        "                new_tokens.append(word)\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "# Function to compute sentiment based on rating and text\n",
        "def compute_sentiment(row):\n",
        "    rating = row['rating']\n",
        "    text = row['text']\n",
        "\n",
        "    # Review length feature\n",
        "    review_length = len(text.split())  # **New Change: Review length as a feature**\n",
        "\n",
        "    # Start with the rating sentiment\n",
        "    if rating in [1, 2]:\n",
        "        sentiment_score = -1  # Likely negative\n",
        "    elif rating in [4, 5]:\n",
        "        sentiment_score = 1  # Likely positive\n",
        "    else:\n",
        "        sentiment_score = 0  # Likely neutral\n",
        "\n",
        "    # Tokenize and analyze the text and word vectors\n",
        "    tokens, word_vectors = preprocess_text(text)\n",
        "\n",
        "    # Handle negations\n",
        "    tokens = handle_negations(tokens)\n",
        "\n",
        "    # Handle mixed feelings with \"but\"\n",
        "    before_but, after_but = handle_but(tokens)\n",
        "\n",
        "    # Adjust the score based on words before and after \"but\"\n",
        "    if before_but:\n",
        "        for word in before_but:\n",
        "            if word in positive_words:\n",
        "                sentiment_score += 1\n",
        "            elif word in negative_words:\n",
        "                sentiment_score -= 1\n",
        "\n",
        "    if after_but:\n",
        "        sentiment_score = 0  # Reset to focus on post-\"but\" sentiment\n",
        "        for word in after_but:\n",
        "            if word in positive_words:\n",
        "                sentiment_score += 1\n",
        "            elif word in negative_words:\n",
        "                sentiment_score -= 1\n",
        "\n",
        "    # Handle short or empty reviews by down-weighting their impact\n",
        "    if review_length < 5:\n",
        "        sentiment_score *= 0.5  # Reduce influence if review is too short\n",
        "\n",
        "    # Return final sentiment (-1 for negative, 0 for neutral, 1 for positive)\n",
        "    if sentiment_score > 0:\n",
        "        return 1\n",
        "    elif sentiment_score < 0:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Apply the sentiment function to your dataset\n",
        "df['computed_sentiment'] = df.apply(compute_sentiment, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result to a new Excel file\n",
        "df.to_excel('/content/processed_reviews_with_improved_sentiment.xlsx', index=False)"
      ],
      "metadata": {
        "id": "TTM2T-uGocuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGdl8DRPadK2",
        "outputId": "845b492e-65a4-4929-8672-bc46016ed658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export as csv\n",
        "csv_file_name = \"Edited_SentimentAnalysisBelle.csv\"\n",
        "csv_folder_path = '/content/drive/MyDrive/DSA3101 Data/Subgroup A/'\n",
        "\n",
        "df.to_csv(csv_folder_path + csv_file_name, index=False)  # exclude index\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA5lc7m0vUVI",
        "outputId": "9a0b274d-fd34-4125-b1da-3b0123627af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    }
  ]
}